# -*- coding: utf-8 -*-
"""AI Mental Fitness Tracker.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b3-Iqtrc1yB_IGxXYE4vanCrfv1vmYOz

# AI Mental Fitness Tracker
- This notebook deals with the 'Mental Fitness' using  different Machine Learning Algorithms

### About the Data
- **Data Overview:** This is a **'Mental Health'**.csv data.

## Import requires libraries
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)

from google.colab import drive
drive.mount('/content/drive')

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

"""## Exploratory Data Analysis

### Load and prepare data
"""

df1 = pd.read_csv("/content/drive/MyDrive/Jupyter Notebooks Projects/AI_Fitness_Tracker/prevalence-by-mental-and-substance-use-disorder.csv")

df2 = pd.read_csv("/content/drive/MyDrive/Jupyter Notebooks Projects/AI_Fitness_Tracker/mental-and-substance-use-as-share-of-disease.csv")

df1.head()

df2.head(10)

data = pd.merge(df1,df2)
data.head(10)

"""### Data Cleaning"""

# Missing value in the dataset
data.isnull().sum()

# Drop the column
data.drop("Code", axis = 1,inplace = True )

# View the data
data.head(10)

# size = row * column,shape = tuple of array dimension(row,column)
data.size,data.shape

# Column set
data.set_axis(['Country', 'Year', 'Schizophrenia', 'Bipolar_disorder','Eating_disorder','Anxiety','drug_usage','depression','alcohol','mental_fitness'],axis ='columns', inplace = True)

data.head(10)

"""### Visualization"""

plt.figure(figsize = (12,6))
sns.heatmap(data.corr(),annot = True,cmap = 'Blues') # Heatmap is define as a graphical representation of data using colors to visualize the value of the matrix
plt.plot()

"""### Takeaway Points
- Eating_disorder is positively correlated to mental_fitness and vice-versa as our eating choice after our mental health.
"""

sns.pairplot(data, corner = True) # pairwise relationship in a dataset
plt.show()

mean = data['mental_fitness'].mean()
mean

fig = px.pie(data, values = 'mental_fitness', names = 'Year')
fig.show()

fig = px.line(data, x="Year", y ="mental_fitness", color = "Country",markers = "True",color_discrete_sequence = ["red","blue"], template = "plotly_dark")
fig.show()

df = data.copy()

# Information about the dataframe
df.info()

# Transform non-numeric labels into numeric labels
from sklearn.preprocessing import LabelEncoder #  LabelEncoder used to normalize labels
l = LabelEncoder()
for i in df.columns:
    if df[i].dtype == 'object':       # transform the non-numerical labels ( as long as they are hashable and comparable) to numerical labels
        df[i] = l.fit_transform(df[i])

df.shape

"""# Split Data - (6840,10)
- In this step, we are going to split data in two parts (traning and testing), so that we can train our model on traning dataset and test its accuracy on unseen test data.
"""

X = df.drop('mental_fitness' ,axis = 1)
y = df['mental_fitness']
from sklearn.model_selection import train_test_split # Use to split the original data into training data & test data
xtrain, xtest ,ytrain ,ytest= train_test_split(X,y, test_size= .20,random_state =2)

# random_state simply sets seed to the random generator, so that your train-test splits are always deterministic.If you don't set seed, it is different each time.

# Training (6840,10)
# 6840*80/100 = 5472
# 6840*20/100 = 1368

print("xtrain: ", xtrain.shape)
print("xtest: ", xtest.shape)
print("ytrain: ", ytrain.shape)
print("ytest: ", ytest.shape)

"""#### **Takeaway Points:**
- Usually we take more and more data in training so it's easy for the model to learn with more data

## Model Training
- As we have done with preprocessing part, it is time to train our model.I am going to train model using Linear Regression and Random Forest Regressor algorithms and then we will compare the performance of these two different models


- **Linear Regression:** Linear regression analysis is used to predit the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.
     
     Y = AX + B
     
     Y: Dependent Variable

     A: Slope

     X: Independent Variable
     
     B: y-intercept
     
- **Random Forest Regressor:** A random forest is a meta estimator that fit a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.**It is used to solve both regression and classification problems.**    

- **XGBoost (Extreme Gradient Boosting):** It is a powerful and efficient machine learning algorithm designed for supervised learning tasks like regression, classification, and ranking. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model. **XGBoost is known for its speed, scalability, and superior performance due to its optimized boosting technique and regularization strategies.**

- **KNN (K-Nearest Neighbors):** It is a simple yet effective machine learning algorithm used for classification and regression tasks. It relies on the principle that data points with similar features tend to have similar target values. In KNN, a new data point's label or value is predicted based on the majority class or average of the K-nearest neighbors in the training data. The value of K determines the number of neighbors considered, and the distance metric (e.g., Euclidean distance) measures their similarity. KNN is non-parametric and easy to understand, making it a versatile choice for various applications, especially when dealing with **small** to **medium-sized datasets.**


- **SVR (Support Vector Regression):** is a machine learning algorithm used for regression tasks. It is an extension of Support Vector Machines (SVM) for continuous target variables. SVR works by finding the hyperplane that best fits the data within a margin (epsilon-tube) defined around the regression line. The objective is to maximize the margin while allowing some points to lie outside the tube, resulting in a robust and flexible model. SVR uses a kernel function to map the input features into a higher-dimensional space, enabling the handling of non-linear relationships. It is particularly useful for small to medium-sized datasets and can effectively capture complex patterns in the data.


- **Decision Trees:** A decision tree is a versatile and intuitive machine learning algorithm used for both classification and regression tasks. It works by recursively partitioning the data based on feature values to create a tree-like structure of decisions. Each internal node represents a feature, and each leaf node represents a class label (in classification) or a predicted value (in regression). The algorithm uses criteria like Gini impurity or entropy to select the best feature to split the data at each step. Decision trees are interpretable, easy to visualize, and can handle both numerical and categorical data. They are popular due to their simplicity and ability to capture complex relationships in the data.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
lr = LinearRegression()
lr.fit(xtrain,ytrain)  # fit training data


# model evaluation for training set
ytrain_pred = lr.predict(xtrain)

# The mean square error is the average of the square of the difference between the observed and predicted values of a variable
mse = mean_squared_error(ytrain, ytrain_pred) # Observed value, predicted value

# Root Mean Square Error measures the average difference between values predicted by a model and the actul values
rmse = (np.sqrt(mean_squared_error(ytrain, ytrain_pred)))

# The coefficient of determination, or R2, is a measure the provides information about the goodness of fit of a model. In the context of regression it is a staatistical measure of
r2 = r2_score(ytrain, ytrain_pred)


print("The Linear Regression model performance for training set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(xtrain,ytrain)

# model evaluation for training set
ytrain_pred = rf.predict(xtrain)

mse = mean_squared_error(ytrain, ytrain_pred)
rmse = (np.sqrt(mean_squared_error(ytrain, ytrain_pred)))

r2 = r2_score(ytrain, ytrain_pred)

print("The Random Forest Regressor model performance for training set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))

from xgboost import XGBRegressor
xboost_model = XGBRegressor()
xboost_model.fit(xtrain,ytrain)

# model evaluation for training set
ytrain_pred = xboost_model.predict(xtrain)

mse = mean_squared_error(ytrain, ytrain_pred)
rmse = (np.sqrt(mean_squared_error(ytrain, ytrain_pred)))

r2 = r2_score(ytrain, ytrain_pred)

print("The XGBoost model performance for training set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))

from sklearn.neighbors import KNeighborsRegressor

# Create and fit the KNN model
knn_model = KNeighborsRegressor(n_neighbors=5)  # You can choose the number of neighbors (k) as per your preference
knn_model.fit(xtrain, ytrain)
ytrain_pred = knn_model.predict(xtrain)

mse = mean_squared_error(ytrain, ytrain_pred)
rmse = (np.sqrt(mean_squared_error(ytrain, ytrain_pred)))

r2 = r2_score(ytrain, ytrain_pred)

print("KNN Model performance for training set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))

from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler

# Scale the data since SVR is sensitive to feature scales
scaler = StandardScaler()
xtrain_scaled = scaler.fit_transform(xtrain)
# ytrain_scaled = ytrain.reshape(-1, 1)  # Reshape ytrain to 2D array if it's 1D

# Create and fit the SVR model
svr_model = SVR(kernel='rbf')  # 'rbf' is a common kernel for non-linear data
svr_model.fit(xtrain_scaled, ytrain)
ytrain_pred = svr_model.predict(xtrain_scaled)
mse = mean_squared_error(ytrain, ytrain_pred)
rmse = (np.sqrt(mean_squared_error(ytrain, ytrain_pred)))

r2 = r2_score(ytrain, ytrain_pred)

print("SVR Model performance for training set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))

from sklearn.tree import DecisionTreeRegressor

# Create and fit the Decision Tree model
tree_model = DecisionTreeRegressor()
tree_model.fit(xtrain, ytrain)
ytrain_pred = tree_model.predict(xtrain)
mse = mean_squared_error(ytrain, ytrain_pred)
rmse = (np.sqrt(mean_squared_error(ytrain, ytrain_pred)))

r2 = r2_score(ytrain, ytrain_pred)

print("Decision Tree Model performance for training set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))

"""## Evaluation
- In this part, we will compare the scores of above two models
"""

# Linear Regression model evaluation for testing set
ytest_pred = lr.predict(xtest)   # using test data (unseen data)
mse = mean_squared_error(ytest,ytest_pred)
rmse = (np.sqrt(mean_squared_error(ytest,ytest_pred)))
r2 = r2_score(ytest, ytest_pred)
print("The Linear Regression model performance for testing set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))
print('\n')



# Random Forest Regressor model evaluation for testing set
ytest_pred = rf.predict(xtest)
mse = mean_squared_error(ytest, ytest_pred)
rmse = (np.sqrt(mean_squared_error(ytest,ytest_pred)))
r2 = r2_score(ytest, ytest_pred)
print("The Random Forest Regressor model performance for testing set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))
print('\n')



# Extreme Gradient Boosting (XGBOOST)
ytest_pred = xboost_model.predict(xtest)
mse = mean_squared_error(ytest, ytest_pred)
rmse = (np.sqrt(mean_squared_error(ytest,ytest_pred)))
r2 = r2_score(ytest, ytest_pred)
print("The XGBOOST model performance for testing set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))
print('\n')


# KNN model evaluation for testing set
ytest_pred = knn_model.predict(xtest)
mse = mean_squared_error(ytest, ytest_pred)
rmse = (np.sqrt(mean_squared_error(ytest,ytest_pred)))
r2 = r2_score(ytest, ytest_pred)
print("KNN Model performance for testing set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))
print('\n')


# SVR model evaluation for testing set
xtest_scaled = scaler.fit_transform(xtest)
ytest_pred = svr_model.predict(xtest_scaled)
mse = mean_squared_error(ytest, ytest_pred)
rmse = (np.sqrt(mean_squared_error(ytest,ytest_pred)))
r2 = r2_score(ytest, ytest_pred)
print("SVR Model performance for testing set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))
print('\n')


# Decision Tree evaluation for testing set
ytest_pred = tree_model.predict(xtest)
mse = mean_squared_error(ytest, ytest_pred)
rmse = (np.sqrt(mean_squared_error(ytest,ytest_pred)))
r2 = r2_score(ytest, ytest_pred)
print("Decision Tree Model performance for testing set")
print("---------------------------------------")
print("MSE is {}".format(mse))
print("RMSE is {}".format(rmse))
print("R2 score is {}".format(r2))

"""#### **Takeaway Points**:
- Random Forest model performs well on both training and testing data    
"""